{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "946de5a0-9584-4b2c-8aa4-36d97c1556c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb8138-1507-42df-8119-e57e43ea631a",
   "metadata": {},
   "source": [
    "# TASK 1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c61cb7-4f28-408d-ab27-5970863cf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # automatic padding and truncation when processing inputs of different sizes\n",
    "        # returns as pytorch tensors\n",
    "        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # attention mask to ignore the padding\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # mean pooling gives avg of vector embeddings, good for looking at similarities, captures context of entire chunk\n",
    "        summed = torch.sum(token_embeddings * attention_mask, dim=1)\n",
    "        counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
    "        mean_pooled = summed / counts\n",
    "\n",
    "        # L2 normalization for our loss function later\n",
    "        embeddings = F.normalize(mean_pooled, p=2, dim=1)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68a93115-cbb7-4f7d-ae9c-720a2d69f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: 768\n",
      "tensor([[ 0.0135, -0.0153, -0.0146,  ...,  0.0093,  0.0225, -0.0329],\n",
      "        [ 0.0243, -0.0455, -0.0531,  ..., -0.0036,  0.0349, -0.0083],\n",
      "        [ 0.0696, -0.0363, -0.0494,  ...,  0.0026,  0.0141, -0.0053],\n",
      "        [-0.0223, -0.0005, -0.0050,  ..., -0.0093, -0.0485,  0.0680],\n",
      "        [-0.0006,  0.0084, -0.0038,  ..., -0.0211, -0.0363,  0.0531]])\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer()\n",
    "model.eval()\n",
    "\n",
    "sentences = [\n",
    "    \"My favorite show right now is Twin Peaks.\",\n",
    "    \"David Lynch was the one who wrote Blue Velvet\",\n",
    "    \"Twin Peaks was written by David Lynch and Mark Frost.\",\n",
    "    \"Every finite language is regular, but not every regular language is finite.\",\n",
    "    \"There are many applications for Context Free Grammars\",\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(sentences)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape[1])\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ead29-7cc3-49ba-a89b-c2ed4e111986",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# TASK 2 #\n",
    "\n",
    "\n",
    "The tasks I want to implement here is sentence classification and Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7828d42f-15cf-4e19-9103-59b66fd7e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, num_ner_labels):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        \n",
    "        # create two classifier heads to handle multitask\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)          # For sentence classification\n",
    "        self.ner_classifier = nn.Linear(hidden_size, num_ner_labels)    # For token-level NER\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # instead of just mean pooling to get fixed size emebeddings, get two different outputs\n",
    "        # for the two tasks\n",
    "        \n",
    "        # CLS is better for sentence classification, the full token sequence is good for NER\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state         # (batch_size, seq_len, hidden_dim)\n",
    "        cls_output = sequence_output[:, 0, :]               # cls token at position 0\n",
    "        \n",
    "        class_logits = self.classifier(cls_output)          # Sentence classification output\n",
    "        ner_logits = self.ner_classifier(sequence_output)   # NER output (token-wise)\n",
    "        \n",
    "        return class_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, class_logits, ner_logits, class_labels, ner_labels, \n",
    "                 classification_weight=1.0, ner_weight=1.0, ner_ignore_index=-100):\n",
    "        \n",
    "        # Classification loss\n",
    "        classification_loss = F.cross_entropy(class_logits, class_labels)\n",
    "        \n",
    "        # NER loss\n",
    "        ner_logits_flat = ner_logits.view(-1, ner_logits.size(-1))\n",
    "        ner_labels_flat = ner_labels.view(-1)\n",
    "        ner_loss = F.cross_entropy(ner_logits_flat, ner_labels_flat, ignore_index=ner_ignore_index)\n",
    "    \n",
    "        # weighted joint loss, default is evenly weighted but option to change depending on finetuning needs or dataset specifics\n",
    "        joint_loss = classification_weight * classification_loss + ner_weight * ner_loss\n",
    "        \n",
    "        return joint_loss, classification_loss, ner_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4fbb2d1-2dec-41bf-977c-3bad64c36ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1679, -0.5082,  0.3046, -0.4215, -0.2039],\n",
      "        [ 0.1733, -0.2227, -0.0356, -0.3514, -0.2043],\n",
      "        [ 0.2149, -0.2663,  0.2406, -0.4135, -0.3215],\n",
      "        [-0.0585, -0.2945,  0.4251, -0.3147, -0.3437],\n",
      "        [ 0.1383, -0.1643,  0.4067, -0.2555, -0.0611]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[[-8.6707e-01,  2.3506e-01,  1.0435e-01,  2.0533e-01,  2.6958e-02,\n",
      "          -1.4714e-01,  1.8400e-01, -5.3831e-01, -1.4410e-01],\n",
      "         [-3.2364e-01,  8.0927e-02, -2.2835e-02,  5.0003e-02,  2.0383e-01,\n",
      "          -2.9610e-01,  6.3430e-02, -2.2567e-02, -9.7748e-02],\n",
      "         [ 2.3590e-01,  4.7415e-01, -1.0983e-01, -2.9346e-02,  2.6646e-01,\n",
      "           3.7116e-02,  1.1024e-01, -5.9155e-02,  6.4788e-01],\n",
      "         [-3.6942e-01,  2.9288e-01, -3.5211e-01, -5.6537e-01,  6.8895e-02,\n",
      "           3.5708e-01,  2.6148e-03,  2.6933e-01, -1.7191e-03],\n",
      "         [ 4.3264e-01,  1.1943e-01, -2.1869e-01,  1.1204e-01, -7.1670e-01,\n",
      "           1.6291e-01, -2.1125e-02, -1.3089e-01, -2.6327e-01],\n",
      "         [ 4.6894e-01,  1.5060e-01,  2.7885e-01,  1.7188e-01, -1.9910e-01,\n",
      "           1.5534e-01, -7.4003e-02, -4.9814e-02,  4.6542e-01],\n",
      "         [ 3.1951e-01,  1.3051e-01, -2.9704e-01,  2.7115e-01,  2.1310e-01,\n",
      "           2.2487e-01, -2.5701e-01,  2.7853e-01,  3.6969e-01],\n",
      "         [-3.0436e-01, -3.0060e-01,  3.7689e-01,  2.0441e-01, -3.3406e-01,\n",
      "           2.9930e-01, -1.5249e-01,  6.1473e-01,  2.9069e-01],\n",
      "         [-4.0517e-01,  1.9763e-01,  1.7460e-01,  3.8825e-01, -1.8607e-01,\n",
      "           9.5301e-02, -6.2775e-01,  5.8628e-01,  1.3230e-01],\n",
      "         [-2.5497e-01,  3.5436e-01, -4.2475e-02, -6.3324e-02,  4.0853e-02,\n",
      "           1.6975e-01, -1.5501e-01,  2.1188e-02,  1.2329e-01],\n",
      "         [-4.3846e-02,  3.9766e-01, -9.2570e-02,  2.2393e-02, -8.0273e-02,\n",
      "           3.8104e-01, -7.4792e-02, -1.3314e-02,  1.0021e-01],\n",
      "         [-2.2953e-01,  2.8427e-01, -2.8662e-01,  3.5855e-02,  1.1610e-01,\n",
      "           1.3621e-01,  7.0222e-02, -1.1694e-02,  7.2125e-02],\n",
      "         [-3.2275e-01,  3.9962e-01, -4.1016e-01,  2.5655e-01,  1.0330e-01,\n",
      "          -1.1268e-01,  5.6294e-02,  5.8727e-02,  5.8597e-02],\n",
      "         [-2.7210e-01,  3.3245e-01, -3.0845e-01,  1.1126e-01,  8.7561e-02,\n",
      "           2.0759e-01,  3.3775e-02, -4.7431e-02,  8.2620e-02],\n",
      "         [-3.6876e-01,  4.5626e-01, -5.1492e-01,  4.0081e-01,  5.7445e-02,\n",
      "          -1.8311e-01, -2.9034e-02,  6.3490e-02,  9.3336e-02],\n",
      "         [-3.6165e-01,  4.0570e-01, -4.2774e-01,  2.1247e-01,  1.4268e-01,\n",
      "          -2.0259e-02,  8.9068e-02,  6.6629e-02,  5.8532e-02]],\n",
      "\n",
      "        [[-7.9817e-01,  1.3725e-01,  9.0754e-02,  1.6975e-01,  2.7615e-02,\n",
      "           1.2264e-03,  2.7485e-01, -2.9033e-01, -2.2869e-01],\n",
      "         [-1.0690e+00,  1.4000e-01,  1.7422e-01,  2.0190e-01,  1.9747e-01,\n",
      "           1.1288e-01,  4.0052e-02, -5.1035e-02, -3.0146e-01],\n",
      "         [-1.0045e+00, -1.8323e-01,  1.6937e-01, -1.6843e-01,  6.7983e-02,\n",
      "           2.0221e-01, -2.4844e-02,  1.9825e-01,  3.8540e-02],\n",
      "         [ 1.9129e-01, -1.1529e-01,  1.8843e-01,  9.8870e-02,  1.1520e-02,\n",
      "           2.3398e-01,  3.8706e-01,  4.1511e-01,  2.1643e-01],\n",
      "         [ 4.7677e-01, -9.4730e-02,  4.4132e-01,  1.2250e-01, -4.8205e-01,\n",
      "           4.5046e-01,  4.8320e-01,  2.0556e-01,  3.1347e-01],\n",
      "         [ 3.7963e-02,  1.2153e-01,  2.9648e-01,  1.1438e-01, -4.9083e-01,\n",
      "          -3.8429e-02,  5.0075e-01,  5.4580e-01,  4.3087e-01],\n",
      "         [ 3.1865e-01,  1.2497e-03,  4.1247e-02,  2.0001e-03, -4.1477e-01,\n",
      "           2.7964e-01,  2.7690e-01,  3.6349e-01,  2.9936e-01],\n",
      "         [-3.5603e-02,  5.1979e-01,  3.7693e-02, -2.7933e-01,  3.6950e-02,\n",
      "           3.5207e-01,  1.8432e-01,  4.1443e-01,  1.1562e-01],\n",
      "         [-1.2194e-01, -3.7672e-01,  3.0637e-01,  5.8039e-01, -2.0785e-01,\n",
      "           6.5631e-01, -5.8329e-01,  1.1822e+00,  2.2636e-01],\n",
      "         [-1.4241e-01,  6.1341e-02,  1.8362e-02,  5.4403e-01, -4.7935e-01,\n",
      "           3.4140e-01, -4.6798e-01,  9.5150e-01,  8.6933e-02],\n",
      "         [-2.1993e-01,  3.2696e-01,  1.8026e-02, -5.0917e-02, -9.5554e-02,\n",
      "           2.0869e-01,  1.2403e-01, -9.7497e-02,  1.2598e-01],\n",
      "         [-3.3956e-01,  1.6442e-01, -2.7139e-01,  7.4288e-02, -3.4289e-02,\n",
      "           4.3542e-02,  8.2847e-02, -8.4391e-03,  1.3417e-01],\n",
      "         [-4.5429e-01,  2.6593e-01, -3.0689e-01,  7.4849e-02, -1.2731e-03,\n",
      "          -8.7334e-02,  5.0387e-02,  1.1152e-01,  1.8688e-01],\n",
      "         [-3.7640e-01,  1.7737e-01, -2.6212e-01,  7.9582e-02, -4.5786e-02,\n",
      "           3.6581e-02,  1.0787e-01, -4.0393e-02,  1.7686e-01],\n",
      "         [-4.7077e-01,  2.0068e-01, -3.2999e-01,  1.3568e-01, -3.9652e-02,\n",
      "          -1.1758e-01,  4.4676e-02,  1.1993e-01,  1.5229e-01],\n",
      "         [-4.4408e-01,  1.9132e-01, -3.3584e-01,  1.6365e-01, -3.5725e-02,\n",
      "          -4.8736e-02,  4.3344e-02,  6.2809e-02,  1.7079e-01]],\n",
      "\n",
      "        [[-9.7384e-01,  1.9014e-01,  1.3948e-01,  3.7147e-01,  3.1568e-01,\n",
      "           1.4566e-02,  2.5305e-01, -1.3735e-01, -2.7027e-01],\n",
      "         [-8.7156e-01,  2.0855e-03,  4.6537e-01,  5.5150e-01, -3.3694e-01,\n",
      "           4.5865e-01,  2.5938e-01,  3.6600e-01, -1.4849e-01],\n",
      "         [-2.8084e-01,  2.8028e-01,  1.5751e-01,  5.5315e-01, -2.4767e-01,\n",
      "           4.6868e-01, -4.3154e-01,  7.8103e-01,  2.3288e-01],\n",
      "         [-4.8779e-02,  4.0329e-01,  3.9780e-01,  5.0586e-01,  2.1955e-01,\n",
      "           6.3981e-01,  2.0457e-01,  7.7813e-01,  2.3162e-02],\n",
      "         [ 3.7389e-01,  6.3344e-01,  2.3758e-01,  7.0480e-02,  2.9283e-01,\n",
      "           3.5429e-01,  1.2591e-01,  2.6175e-01,  1.7702e-01],\n",
      "         [ 4.4322e-01,  4.9884e-01, -2.5218e-01,  3.5341e-01,  5.1865e-02,\n",
      "           2.0152e-01,  1.7671e-01,  2.3326e-02,  4.4839e-01],\n",
      "         [-3.4403e-01,  3.5027e-01, -2.3416e-01,  3.7692e-01,  3.2821e-01,\n",
      "           3.8967e-01,  1.1905e-01,  1.3174e-01,  3.4932e-01],\n",
      "         [-6.7228e-01, -5.0922e-02, -3.3976e-02,  9.2405e-03,  4.6404e-01,\n",
      "           3.4678e-01,  3.2980e-02,  3.6802e-02,  3.5271e-01],\n",
      "         [-8.3544e-02,  2.8756e-01, -4.6007e-04,  2.6109e-02,  2.4238e-01,\n",
      "           1.8162e-01,  1.9233e-01,  8.1426e-02,  1.3791e-01],\n",
      "         [-7.8764e-02,  2.3366e-01, -1.6957e-01,  2.9896e-01,  1.5599e-01,\n",
      "           2.5013e-01, -1.4311e-01,  1.4687e-01,  3.4381e-01],\n",
      "         [ 1.5566e-01,  1.5941e-01,  1.8251e-01,  2.9781e-01,  1.8652e-01,\n",
      "           4.4449e-01, -1.5478e-01,  2.5843e-02,  4.5030e-01],\n",
      "         [-1.9360e-01,  3.6366e-01,  3.7127e-03,  2.9110e-02,  2.0318e-02,\n",
      "           2.9168e-01, -1.9409e-02, -4.1334e-02,  1.5450e-01],\n",
      "         [-5.2557e-02,  3.5315e-01,  8.5837e-02,  7.0137e-03, -3.4636e-02,\n",
      "           3.5811e-01, -3.0974e-02, -7.0381e-02,  6.8354e-02],\n",
      "         [-1.2210e-01,  3.2163e-01, -3.3932e-02, -2.8592e-02,  1.8391e-01,\n",
      "           1.6263e-01,  2.4774e-01,  1.4791e-01,  5.3672e-02],\n",
      "         [-1.2803e-01,  3.9294e-01, -1.0368e-01, -8.2006e-02,  3.3386e-01,\n",
      "           7.6373e-02,  1.5551e-01,  3.6969e-02, -3.7377e-02],\n",
      "         [-9.1847e-02,  3.7624e-01, -5.7465e-02,  5.5796e-02,  1.4817e-01,\n",
      "           2.0432e-01,  1.9608e-01,  1.5815e-01,  1.0531e-01]],\n",
      "\n",
      "        [[-8.4738e-01,  1.5016e-02, -1.6797e-01,  6.5620e-02,  2.5092e-01,\n",
      "          -4.4893e-01,  4.9475e-02,  4.4983e-04, -4.4293e-01],\n",
      "         [-3.0538e-01, -1.5299e-01,  9.6459e-02,  2.0868e-01,  1.6852e-01,\n",
      "          -1.5441e-01, -4.3490e-01,  8.6962e-01, -4.9770e-01],\n",
      "         [-3.7306e-01, -1.5409e-01,  2.5810e-01,  2.2325e-01, -2.0473e-02,\n",
      "          -5.4619e-02, -2.5563e-01,  8.4068e-01, -1.5518e-01],\n",
      "         [-3.4852e-01, -1.0206e-01,  1.7497e-01,  2.2929e-01, -3.0875e-01,\n",
      "          -8.8666e-02, -2.4841e-01,  7.7670e-01, -7.2656e-02],\n",
      "         [ 1.9968e-01, -1.9528e-01, -3.3481e-01,  3.9794e-01,  3.9348e-01,\n",
      "           6.0566e-02, -3.4157e-01,  9.0196e-01, -1.6265e-01],\n",
      "         [-1.3365e-01, -3.4422e-01,  3.5021e-01, -4.3250e-01,  1.5630e-01,\n",
      "           1.1849e-01, -2.8025e-01,  9.7449e-01, -2.4403e-01],\n",
      "         [-8.1862e-02,  4.1793e-01,  6.7741e-02, -2.0160e-01,  8.4678e-02,\n",
      "           2.4446e-01,  1.4983e-01, -3.9131e-02,  1.6685e-03],\n",
      "         [-1.5303e-01, -1.7966e-01, -1.2496e-01, -1.6750e-01,  9.0868e-02,\n",
      "           3.8796e-01, -5.7230e-01,  4.8644e-01,  2.9568e-01],\n",
      "         [ 2.0729e-01, -6.4721e-01, -4.3891e-01, -1.9376e-01,  2.8862e-01,\n",
      "           2.6467e-01, -3.3490e-01,  5.1400e-01, -1.7751e-01],\n",
      "         [ 2.8270e-01, -1.5199e-01,  7.7683e-02,  1.5244e-01,  1.4236e-01,\n",
      "          -1.6965e-02, -1.7906e-01,  7.9071e-01, -2.6040e-01],\n",
      "         [-1.4405e-03, -2.6621e-01,  2.1252e-01, -1.1645e-01,  1.9310e-01,\n",
      "          -1.3046e-01, -1.4338e-01,  9.7178e-01, -3.3234e-01],\n",
      "         [-1.5258e-01, -1.4837e-01,  2.4380e-01,  1.7539e-01, -2.5760e-01,\n",
      "          -2.1551e-01, -7.5377e-02,  6.8583e-01, -1.3674e-01],\n",
      "         [ 3.2508e-01, -1.2181e-01, -2.3239e-01,  2.2684e-01,  4.2970e-01,\n",
      "           4.9362e-02, -2.8947e-01,  7.6083e-01, -2.4647e-01],\n",
      "         [-2.0175e-01, -3.3211e-01,  3.3365e-01,  6.1017e-02,  1.1049e-01,\n",
      "          -4.0529e-02, -1.5421e-01,  7.0142e-01, -2.8291e-01],\n",
      "         [-3.0114e-01,  3.3159e-01, -3.2334e-02, -9.6546e-02, -5.8715e-03,\n",
      "           1.0412e-01, -3.7404e-02, -2.0360e-02,  1.3274e-01],\n",
      "         [-6.8278e-02, -1.1041e-01, -2.8285e-02,  1.6501e-01, -2.0705e-01,\n",
      "           8.9879e-02, -3.8971e-01,  9.2811e-01, -9.3877e-02]],\n",
      "\n",
      "        [[-8.8432e-01, -5.7054e-02, -3.8043e-02, -3.3157e-02,  1.1648e-01,\n",
      "          -4.7106e-01,  2.6002e-01, -1.5411e-01, -3.9166e-01],\n",
      "         [-5.4225e-01,  1.2103e-01,  8.2532e-02,  1.9671e-01,  1.5541e-02,\n",
      "          -1.2529e-01, -5.0653e-03,  2.3830e-01, -5.2843e-01],\n",
      "         [ 4.4807e-02,  3.7033e-01, -2.6970e-01,  8.4832e-02, -2.8075e-01,\n",
      "           1.3076e-01, -3.7184e-02,  7.5798e-01, -2.5874e-01],\n",
      "         [-2.5238e-01,  1.4482e-01,  1.3076e-01,  4.1647e-02, -1.6026e-01,\n",
      "           3.3957e-01, -3.2728e-01,  7.9200e-01,  3.1079e-01],\n",
      "         [-1.7352e-01, -8.1933e-02, -9.6386e-02, -2.2681e-01,  4.5175e-01,\n",
      "           3.6386e-02, -2.0302e-01,  7.0479e-01, -4.0983e-02],\n",
      "         [-2.2376e-02,  3.1970e-02, -2.7703e-01,  2.3675e-02,  1.8395e-01,\n",
      "           7.2726e-03, -9.1389e-02,  6.3823e-01,  7.0801e-02],\n",
      "         [ 1.4950e-01, -2.4767e-02, -1.1923e-01, -4.5187e-01, -2.5948e-02,\n",
      "           1.0538e-01,  2.1955e-01,  9.3658e-02, -7.3588e-02],\n",
      "         [-1.2800e-01, -5.8386e-01, -2.2348e-01, -5.4611e-02,  2.1239e-01,\n",
      "          -2.4258e-01, -1.5093e-03,  3.0029e-01,  1.0898e-01],\n",
      "         [-1.2949e-01, -3.4772e-01,  4.2152e-01, -7.3308e-02, -1.4054e-01,\n",
      "          -1.7744e-01, -1.4636e-01,  2.3331e-01,  4.3297e-01],\n",
      "         [-3.4727e-01,  1.0335e-01, -3.4512e-01,  4.1768e-01,  1.0012e-01,\n",
      "          -3.5389e-01, -9.5541e-02,  5.1575e-01,  3.6471e-01],\n",
      "         [-2.2582e-01,  3.5990e-01, -7.3934e-02, -1.9170e-01, -9.0454e-02,\n",
      "           1.5902e-01,  5.9410e-03, -4.3314e-02,  1.4201e-01],\n",
      "         [-1.5421e-01,  2.4807e-01, -1.5457e-01,  6.2712e-02,  9.2481e-02,\n",
      "          -2.9634e-01,  6.7365e-02,  2.6369e-01, -5.3956e-03],\n",
      "         [-2.1685e-01,  3.4797e-01, -3.0185e-01,  2.0590e-01,  1.0836e-01,\n",
      "          -4.4594e-01, -1.0248e-02,  3.1089e-01, -4.7470e-02],\n",
      "         [-1.2041e-01,  2.4211e-01, -2.6009e-01,  1.1066e-01,  9.5216e-02,\n",
      "          -2.5569e-01,  3.9775e-02,  2.5839e-01,  1.9476e-02],\n",
      "         [-2.7616e-01,  3.5102e-01, -3.7663e-01,  2.7002e-01,  8.9490e-02,\n",
      "          -5.2440e-01, -4.3388e-02,  3.0067e-01, -3.3755e-02],\n",
      "         [-1.3616e-01,  2.3787e-01, -2.4064e-01,  7.9836e-02,  1.3093e-01,\n",
      "          -3.1322e-01,  3.9413e-02,  2.8910e-01,  6.7553e-03]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = [\n",
    "    \"My favorite show right now is Twin Peaks.\",\n",
    "    \"David Lynch was the one who wrote Blue Velvet\",\n",
    "    \"Twin Peaks was written by David Lynch and Mark Frost.\",\n",
    "    \"Every finite language is regular, but not every regular language is finite.\",\n",
    "    \"There are many applications for Context Free Grammars\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = MultiTaskModel(\"bert-base-uncased\", num_classes=5, num_ner_labels=9)\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "class_logits, ner_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(class_logits)  # (batch_size, num_classes)\n",
    "print(ner_logits)    # (batch_size, seq_len, num_ner_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b112a796-0810-4cfd-9c78-a9ea6430ccf2",
   "metadata": {},
   "source": [
    "# TASK 3 #\n",
    "\n",
    "# Discuss the implications and advantages of each scenario and make sure to explain your rationale as to how the model should be trained given the following: #\n",
    "\n",
    "### If the entire network was frozen, ###\n",
    "\n",
    "then we are relying entirely on the weights that are given on the pretrained model. The advantage would be that training is extremely quick and very low in computational costs, but the disadvantage would be that it can't be trained to do anything new. The rationale behind this is if the task we want to do is in a very similar domain to the current network, so it would already perform well enough for our needs.\n",
    "\n",
    "## If we only froze the transformer backbone, ##\n",
    "\n",
    "then we would only be training the task specific heads by leaving them unfrozen during training. The training would be relatively quick and we are able to train the task specific heads to our liking. The rationale behind this would be if our task is related but not identical to the data the transformer backbone was trained on, and also if we don't have very large amounts of data to be worth training the entire network for.\n",
    "\n",
    "## If we froze only one task-specific head, ##\n",
    "\n",
    "then only one head would continue learning depending on what we chose to not freeze. The advantage and rationale behind this is if one task is already performing well enough, then we can protect/preserve its performance while improving the performance of the other task specific head.\n",
    "\n",
    "#  Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process\n",
    "\n",
    "## Choice of pretrained model ##\n",
    "\n",
    "The choice of the pretrained model would depend on what domain the task we are trying to complete is in. Lets say we want to train a task-specific head that can do Named Entity Recognition on the names of medicines. We should choose something that is related to that domain, so we can leverage off of a pretrained model like Med-BERT that is trained on medical texts already\n",
    "\n",
    "## Layers to freeze/unfreeze ## \n",
    "\n",
    "Unless we have a large enough amount of data to train on to justify unfreezing layers of the pretrained model, we would want to keep the backbone frozen while we unfreeze the task-specific head in order for it to learn. One of the main reasons why transfer learning is beneficial is that it is a solution when it comes to the problem of getting large enough datasets. By utilizing the fact that the pretrained models are trained on much larger datasets beforehand, we can speed up our training and rely on a smaller dataset without worrying too much about overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd33803-4b7d-4b3d-bcf9-79b365a7db1d",
   "metadata": {},
   "source": [
    "\n",
    "# TASK 4\n",
    "\n",
    "#### Here would be a hypothetical training loop for our model, assuming we have a real dataset on hand and we created a dataset class to be loaded using the \n",
    "\n",
    "#### DataLoader. This is probably just a bit more than a bare skeleton for a training loop, as we can also add stuff like scheduling for the learning rate or even \n",
    "\n",
    "#### utilize something like Weights and Biases/WandB to make searching and finetuning the hyperparameters very easy and convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f554247-3520-4132-b8be-f71944925c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "def train_mtl_model(model, train_dataloader, val_dataloader, optimizer, device, \n",
    "                    num_epochs, classification_weight=1.0, ner_weight=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: MultiTaskModel instance\n",
    "        train_dataloader: DataLoader for training data\n",
    "        val_dataloader: DataLoader for validation data\n",
    "        optimizer: torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: number of training epochs\n",
    "        classification_weight: weight for classification loss\n",
    "        ner_weight: weight for NER loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device) # cuda if available if not then CPU\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0 \n",
    "    patience = 5    # early stopping\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        train_class_correct = 0\n",
    "        train_class_total = 0\n",
    "        train_ner_correct = 0\n",
    "        train_ner_total = 0\n",
    "        \n",
    "        # For accumulating true/pred labels across batches for macro metrics, important because we have multiple tasks, so we can see if \n",
    "        #  a certain task is performing better/worse than the other\n",
    "        all_class_true = []\n",
    "        all_class_pred = []\n",
    "        all_ner_true = []\n",
    "        all_ner_pred = []\n",
    "\n",
    "        #         tqdm just for visual clarity although im not even actually training it\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "            \n",
    "            # Unpack batch data assuming the data is in this format\n",
    "            # batch = {input_ids, attention_mask, class_labels, ner_labels}\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            class_labels = batch['class_labels'].to(device)\n",
    "            ner_labels = batch['ner_labels'].to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()  # reset gradients from previous batch\n",
    "            \n",
    "            #Forward pass returns two outputs since it computes both classification and NER outputs\n",
    "            class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Compute joint multi-task loss\n",
    "            loss, class_loss, ner_loss = model.compute_loss(\n",
    "                class_logits, ner_logits, class_labels, ner_labels,\n",
    "                classification_weight, ner_weight\n",
    "            )\n",
    "            \n",
    "            # Backward pass, compute gradients\n",
    "            loss.backward()  # Backpropagate the joint loss\n",
    "            \n",
    "            # Optimizer step, update model parameters\n",
    "            optimizer.step()  \n",
    "\n",
    "            # Accumulate loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy for both tasks\n",
    "            # Classification task\n",
    "            _, class_preds = torch.max(class_logits, 1)\n",
    "            train_class_correct += (class_preds == class_labels).sum().item()\n",
    "            train_class_total += class_labels.size(0)\n",
    "            \n",
    "            # NER task (ignore padded tokens with label -100)\n",
    "            ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "            active_tokens = (ner_labels != -100)  # mask for non-padded tokens\n",
    "            train_ner_correct += ((ner_preds[active_tokens] == ner_labels[active_tokens]).sum().item())\n",
    "            train_ner_total += active_tokens.sum().item()\n",
    "            \n",
    "            # Store for macro metrics\n",
    "            all_class_true.extend(class_labels.cpu().numpy())\n",
    "            all_class_pred.extend(class_preds.cpu().numpy())\n",
    "            \n",
    "            # For NER, only consider non-padded tokens\n",
    "            masked_ner_true = ner_labels[active_tokens].cpu().numpy()\n",
    "            masked_ner_pred = ner_preds[active_tokens].cpu().numpy()\n",
    "            all_ner_true.extend(masked_ner_true)\n",
    "            all_ner_pred.extend(masked_ner_pred)\n",
    "\n",
    "        ## Here we can begin to do the validation part\n",
    "        val_metrics = {\n",
    "            'loss': 0.0,\n",
    "            'class_correct': 0,\n",
    "            'class_total': 0,\n",
    "            'ner_correct': 0,\n",
    "            'ner_total': 0,\n",
    "            'class_true': [],\n",
    "            'class_pred': [],\n",
    "            'ner_true': [],\n",
    "            'ner_pred': []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():  # validation part of the loop\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                class_labels = batch['class_labels'].to(device)\n",
    "                ner_labels = batch['ner_labels'].to(device)\n",
    "                \n",
    "                # forward pass no backprop\n",
    "                class_logits, ner_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                # Loss calculation\n",
    "                loss, _, _ = model.compute_loss(\n",
    "                    class_logits, ner_logits, class_labels, ner_labels,\n",
    "                    classification_weight, ner_weight\n",
    "                )\n",
    "                val_metrics['loss'] += loss.item()\n",
    "                \n",
    "                # Classification metrics\n",
    "                _, class_preds = torch.max(class_logits, 1)\n",
    "                val_metrics['class_correct'] += (class_preds == class_labels).sum().item()\n",
    "                val_metrics['class_total'] += class_labels.size(0)\n",
    "                \n",
    "                # NER metrics\n",
    "                ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "                active_tokens = (ner_labels != -100)\n",
    "                val_metrics['ner_correct'] += ((ner_preds[active_tokens] == ner_labels[active_tokens]).sum().item())\n",
    "                val_metrics['ner_total'] += active_tokens.sum().item()\n",
    "                \n",
    "                # Store for macro metrics\n",
    "                val_metrics['class_true'].extend(class_labels.cpu().numpy())\n",
    "                val_metrics['class_pred'].extend(class_preds.cpu().numpy())\n",
    "                val_metrics['ner_true'].extend(ner_labels[active_tokens].cpu().numpy())\n",
    "                val_metrics['ner_pred'].extend(ner_preds[active_tokens].cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        avg_val_loss = val_metrics['loss'] / len(val_dataloader)\n",
    "        val_class_acc = val_metrics['class_correct'] / val_metrics['class_total']\n",
    "        val_ner_acc = val_metrics['ner_correct'] / val_metrics['ner_total']\n",
    "        val_class_f1 = f1_score(val_metrics['class_true'], val_metrics['class_pred'], average='macro')\n",
    "        val_ner_f1 = f1_score(val_metrics['ner_true'], val_metrics['ner_pred'], average='macro')\n",
    "        \n",
    "        # Print epoch stats\n",
    "        print(f\"\\nEpoch {epoch + 1} Validation Results:\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f} | Class Acc: {val_class_acc:.4f} | NER Acc: {val_ner_acc:.4f}\")\n",
    "        print(f\"Class F1: {val_class_f1:.4f} | NER F1: {val_ner_f1:.4f}\")\n",
    "\n",
    "        \n",
    "        # Save best model and/or\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), 'best_mtl_model.pt')\n",
    "            print(\"New best model saved!\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times > patience:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
